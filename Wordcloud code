!pip install lxml
!pip install wordcloud
!pip install nltk
import random
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
%matplotlib inline

import seaborn as sns

from sklearn import (linear_model, metrics, neural_network, pipeline, preprocessing, model_selection)

from bs4 import BeautifulSoup
import nltk
import string
from nltk.tokenize import RegexpTokenizer
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('wordnet')

from wordcloud import WordCloud

types = pd.read_csv("MBTI_type.csv")
posts = pd.read_csv("MBTI_posts.csv")

sep_posts = posts['posts'].str.split("\|\|\|", expand = True)

def shift(row):
    row = row.dropna()
    row = pd.Series(row.values)
    return row

def remove_first_apostrophe(s):
    if s[0] == "'":
        post = s[1:]
    else:
        post = s
    return post

sep_posts = sep_posts.apply(lambda x: x.str.strip()).replace('', np.nan).dropna(how='all', axis=1)
sep_posts2 = sep_posts.apply(shift, axis = 1)
sep_posts2[0] = sep_posts2[0].apply(remove_first_apostrophe)
sep_posts2.iloc[:, -1] = sep_posts2.iloc[:, -1].apply(lambda x: x[:-1] if isinstance(x, str) else x)
sample_posts = pd.DataFrame(sep_posts2[0])
data = pd.concat([types, sep_posts2], axis=1)


# WORDCLOUDS AND BAR CHARTS OF 10 COMMONLY USED WORDS
# Creating a dataframe for the wordclouds
data_wc = pd.concat([types, sample_posts], axis=1)
data_wc.columns = ['type', 'posts']
data_wc['favorite world'] = data_wc['type'].apply(lambda x: 'Extrovert' if x[0] == 'E' else 'Introvert')
data_wc['information'] = data_wc['type'].apply(lambda x: 'Intuitive' if x[1] == 'N' else 'Sensing')
data_wc['decisions'] = data_wc['type'].apply(lambda x: 'Thinking' if x[2] == 'T' else 'Feeling')
data_wc['structure'] = data_wc['type'].apply(lambda x: 'Judging' if x[3] == 'J' else 'Perceiving')

# Remove stopwords using the 'english' library which includes words such as (the, a, an, is, to)
stopwords = set(nltk.corpus.stopwords.words('english'))
stopwords = stopwords.union(set(string.punctuation))
wnl = nltk.WordNetLemmatizer()

# Defining a function that cleans, tokenizes, removes stopwords, and lemmatizes words in a text
def text_prep(txt):
    soup = BeautifulSoup(txt, "lxml")
    [s.extract() for s in soup('style')]
    txt=soup.text
    txt = txt.lower()
    tokens = [token for token in nltk.tokenize.word_tokenize(txt)]
    tokens = [token for token in tokens if not token in stopwords]
    tokens = [wnl.lemmatize(token) for token in tokens]
    if (len(tokens)==0):
        tokens = ["EMPTYSTRING"]
    return(tokens)

# Defining a function that generates a wordcloud
def gen_wc(posts, title):
    wordcloud = WordCloud(background_color='white', max_words=100).generate(posts)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)
    plt.show()


# Creating a wordcloud and bar chart of 10 most commonly used words for each type in MBTI personality types
letter = ["Extrovert", "Introvert", "Intuitive", "Sensing", "Thinking", "Feeling", "Judging", "Perceiving"]
types = ["favorite world", "favorite world", "information", "information", "decisions", "decisions", "structure", "structure"]

fig = plt.figure(figsize=(10, 60))
spec = fig.add_gridspec(ncols=2, nrows=8)

i = 0
while i < 8:
    for l, t in zip(letter, types):
        ax0 = fig.add_subplot(spec[i, 0])
        text = pd.Series(' '.join(data_wc.groupby(f"{t}")['posts'].get_group(f"{l}")).split()).to_string(index = False)
        text = text_prep(text)
        text_str = ' '.join(text)
        wordcloud = WordCloud(collocations = False, background_color='white', max_words=200).generate(text_str)
        ax0.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        
        ax1 = fig.add_subplot(spec[i, 1])
        post_dict = wordcloud.process_text(text_str)
        word_freq={y: s for y, s in sorted(post_dict.items(),reverse=True, key=lambda item: item[1])}
        dist = pd.DataFrame(list(word_freq.items())[:10])
        dist.columns = ["Words", "Count"]
        ax1.bar(x = dist["Words"], height = dist["Count"])
        ax1.tick_params(axis='x', labelrotation = 45)
        ax1.set_xlabel(f"Top 10 Most Used Words by {l} Types")
        ax1.set_ylabel("Count")
        plt.tight_layout()
        plt.title(f"{l}", x = 0, y=1.05, fontsize = 15)
        i+=1
    plt.show()

